{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2fda7b3-f7ae-47e0-b9ff-9ec9cbffbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = {\n",
    "    \"finance\": [\"stock\", \"market\", \"downturn\", \"investment\", \"loan\", \"credit\", \"insurance\", \"interest rates\", \"forex\", \"debt relief\", \"mortgage\", \"lottery\", \"inheritance\", \"refund\", \"bonus\", \"cryptocurrency\", \"bitcoin\", \"profit\", \"dividend\", \"tax\"],\n",
    "    \"technology\": [\"iphone\", \"innovative\", \"features\", \"software\", \"gadget\", \"AI\", \"blockchain\", \"download\", \"hack\", \"update\", \"security\", \"malware\", \"password\", \"phishing\", \"tech support\", \"app\", \"subscription\", \"virtual\", \"cloud\"],\n",
    "    \"weather\": [\"weather\", \"sunny\", \"rain\", \"storm\", \"forecast\", \"hurricane\", \"temperature\", \"climate\", \"flood\", \"seasonal\", \"tornado\", \"drought\", \"disaster\", \"windy\", \"cold front\"],\n",
    "    \"sports\": [\"football\", \"match\", \"thrilling\", \"basketball\", \"soccer\", \"tennis\", \"tournament\", \"championship\", \"team\", \"score\", \"game\", \"coach\", \"training\", \"Olympics\", \"player\", \"league\"],\n",
    "    \"health\": [\"diet\", \"exercise\", \"medication\", \"supplement\", \"cure\", \"weight loss\", \"disease\", \"wellness\", \"fitness\", \"healthcare\", \"treatment\", \"therapy\", \"pain relief\", \"mental health\", \"workout\", \"nutrition\"],\n",
    "    \"shopping\": [\"discount\", \"coupon\", \"deal\", \"promo\", \"free\", \"offer\", \"sale\", \"voucher\", \"clearance\", \"price\", \"bargain\", \"shopping\", \"exclusive\", \"cashback\", \"buy one get one\", \"limited time\", \"membership\"],\n",
    "    \"travel\": [\"flight\", \"vacation\", \"hotel\", \"cruise\", \"ticket\", \"package\", \"tour\", \"holiday\", \"destination\", \"cheap flights\", \"booking\", \"getaway\", \"airline\", \"passport\", \"visa\"],\n",
    "    \"education\": [\"course\", \"certificate\", \"degree\", \"exam\", \"tutoring\", \"training\", \"scholarship\", \"study\", \"admission\", \"learning\", \"e-learning\", \"webinar\", \"online classes\", \"college\", \"program\", \"assignment\"],\n",
    "    \"real_estate\": [\"property\", \"home\", \"real estate\", \"mortgage\", \"house\", \"apartment\", \"rent\", \"buy\", \"sell\", \"valuation\", \"foreclosure\", \"land\", \"investment property\", \"condo\", \"realtor\"],\n",
    "    \"job_offers\": [\"job\", \"career\", \"opportunity\", \"hiring\", \"resume\", \"recruitment\", \"salary\", \"position\", \"work from home\", \"freelance\", \"apply now\", \"internship\", \"headhunting\", \"vacancy\", \"benefits\"],\n",
    "    \"entertainment\": [\"movie\", \"concert\", \"event\", \"music\", \"streaming\", \"ticket\", \"festival\", \"show\", \"drama\", \"series\", \"celebrity\", \"download\", \"gaming\", \"comedy\", \"premiere\"],\n",
    "    \"gambling\": [\"casino\", \"bet\", \"jackpot\", \"poker\", \"win\", \"wager\", \"bingo\", \"lottery\", \"prize\", \"spin\", \"slot machine\", \"blackjack\", \"roulette\", \"cashout\"],\n",
    "    \"adult_content\": [\"dating\", \"romance\", \"explicit\", \"adult\", \"XXX\", \"escort\", \"singles\", \"chat\", \"hookup\", \"relationship\", \"flirting\", \"intimacy\", \"sexy\", \"nude\", \"privacy\"],\n",
    "    \"scams\": [\"urgent\", \"immediate\", \"winner\", \"guarantee\", \"free\", \"claim now\", \"exclusive\", \"risk-free\", \"limited time\", \"act now\", \"confidential\", \"donation\", \"gift\", \"lottery\", \"fraud\", \"suspicious\", \"verify\", \"alert\"],\n",
    "    \"pharmaceuticals\": [\"medication\", \"pharmacy\", \"drug\", \"prescription\", \"generic\", \"viagra\", \"painkiller\", \"treatment\", \"supplement\", \"pill\", \"health\", \"cure\", \"remedy\", \"medicine\"],\n",
    "    \"luxury\": [\"luxury\", \"branded\", \"watch\", \"jewelry\", \"perfume\", \"designer\", \"exclusive\", \"high-end\", \"yacht\", \"premium\", \"gold\", \"diamond\", \"exclusive\", \"opulent\"],\n",
    "    \"relationships\": [\"friendship\", \"love\", \"relationship\", \"connection\", \"marriage\", \"proposal\", \"partner\", \"dating\", \"romantic\", \"soulmate\", \"breakup\", \"couple\"],\n",
    "    \"finance_scams\": [\"inheritance\", \"refund\", \"bank account\", \"transfer\", \"claim\", \"wire transfer\", \"check\", \"million dollars\", \"compensation\", \"money order\"],\n",
    "    \"urgent_requests\": [\"asap\", \"immediate action\", \"respond quickly\", \"time-sensitive\", \"urgent\", \"important\", \"response needed\", \"deadline\", \"attention\"],\n",
    "    \"charity\": [\"donation\", \"help\", \"fundraising\", \"relief\", \"support\", \"charitable\", \"nonprofit\", \"cause\", \"volunteer\", \"contribute\"],\n",
    "    \"software\": [\"antivirus\", \"download\", \"malware\", \"subscription\", \"license\", \"patch\", \"renewal\", \"update\", \"activation\", \"cracked version\"],\n",
    "    \"investment\": [\"ROI\", \"crypto\", \"trading\", \"stocks\", \"bonds\", \"real estate\", \"portfolio\", \"returns\", \"dividends\", \"mutual fund\", \"ETF\"],\n",
    "    \"fashion\": [\"clothing\", \"brand\", \"designer\", \"style\", \"shoes\", \"handbags\", \"apparel\", \"wardrobe\", \"accessories\", \"sale\", \"limited edition\"],\n",
    "    \"government\": [\"passport\", \"visa\", \"immigration\", \"policy\", \"tax\", \"stimulus\", \"grants\", \"legislation\", \"laws\", \"compliance\", \"form submission\"],\n",
    "    \"personal_services\": [\"cleaning\", \"repair\", \"consultation\", \"advice\", \"coaching\", \"wellness\", \"beauty\", \"appointments\", \"booking\", \"therapy\"],\n",
    "    \"cryptocurrency\": [\"bitcoin\", \"ethereum\", \"blockchain\", \"crypto\", \"mining\", \"wallet\", \"token\", \"NFT\", \"exchange\", \"investment\", \"decentralized\"],\n",
    "    \"scientific_reports\": [\"research\", \"paper\", \"article\", \"experiment\", \"data\", \"analysis\", \"results\", \"supervisor\", \"thesis\", \"dissertation\", \"peer review\", \"publication\", \"proposal\", \"methodology\", \"findings\", \"journal\", \"submission\", \"revision\",\"material\",\"report\",\"data\",\"update\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e63c0aa7-5888-49da-aa20-1eb3ec6a68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_flows = {\n",
    "    \"cause_and_effect\": [\n",
    "        \"because\", \"since\", \"as a result\", \"therefore\", \"consequently\", \n",
    "        \"due to\", \"thus\", \"hence\", \"leads to\", \"results in\", \"is caused by\"\n",
    "    ],\n",
    "    \"comparison_contrast\": [\n",
    "        \"similarly\", \"likewise\", \"on the other hand\", \"however\", \"in contrast\", \n",
    "        \"whereas\", \"while\", \"although\", \"yet\", \"unlike\"\n",
    "    ],\n",
    "    \"chronological_sequential\": [\n",
    "        \"first\", \"next\", \"then\", \"after\", \"subsequently\", \"finally\", \n",
    "        \"before\", \"meanwhile\", \"during\", \"at the same time\"\n",
    "    ],\n",
    "    \"problem_and_solution\": [\n",
    "        \"problem\", \"solution\", \"resolve\", \"address\", \"issue\", \n",
    "        \"remedy\", \"solve\", \"fix\", \"answer\", \"approach\"\n",
    "    ],\n",
    "    \"general_to_specific\": [\n",
    "        \"for example\", \"for instance\", \"such as\", \"including\", \"to illustrate\", \n",
    "        \"in particular\", \"namely\", \"specifically\", \"e.g.\", \"i.e.\"\n",
    "    ],\n",
    "    \"definition_and_description\": [\n",
    "        \"is\", \"refers to\", \"means\", \"defines\", \"is characterized by\", \n",
    "        \"involves\", \"can be described as\", \"is known as\", \"represents\", \"signifies\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35d549-7af4-4596-862a-b3be46a42741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "######################################################\n",
    "# Part 1 : Data Preprocessing\n",
    "######################################################\n",
    "\n",
    "#### Part 1.1 : Data Importing\n",
    "##########################################\n",
    "#data_directory=r'G:\\My Drive\\Phd Courses\\CS 5841- Machine Learning\\Assignments\\1\\spam_ham_dataset - Copy.csv'\n",
    "data_directory=r'G:\\My Drive\\Phd Courses\\CS 5841- Machine Learning\\Assignments\\1\\spam_ham_dataset.csv'\n",
    "\n",
    "imported_data=pd.read_csv(data_directory)\n",
    "\n",
    "# converting to pandas data frame to give futher data manipulation capability\n",
    "#imported_data=imported_data.DataFrame(imported_data)\n",
    "\n",
    "\n",
    "# Initial Inspection for gaining insight about data structure\n",
    "first_5rows=imported_data.iloc[:5,:]\n",
    "print(first_5rows)\n",
    "\n",
    "#### Part 1.2 : Data Cleaning\n",
    "##########################################\n",
    "imported_data_size=imported_data.shape\n",
    "print(imported_data_size)\n",
    "\n",
    "\n",
    "# recognizing problematic data\n",
    "problematic_data=[]\n",
    "for i in range(imported_data_size[0]):\n",
    "    for j in range(imported_data_size[1]): \n",
    "        if imported_data.iloc[i,j]=='Nan':\n",
    "            print(\"There are some NAN values in imported data\")\n",
    "            #print(\"Data cleaning started\")\n",
    "            problematic_data.append([i,j])\n",
    "\n",
    "# Cleaning problematic data: modification if possible \n",
    "for i in range(len(problematic_data)):\n",
    "    temp_data=problematic_data[i]\n",
    "\n",
    "    # we replace if the number of words is Nan by median values of the column\n",
    "    if temp_data[1]==0: # specifying number of words in email column\n",
    "        print (\"yes\")\n",
    "        imported_data.iloc[:,0]=pd.to_numeric(imported_data.iloc[:,0],errors='coerce')\n",
    "        mean_value=imported_data.iloc[:,0].mean(skipna=True)\n",
    "        imported_data.iloc[temp_data[0],0]=mean_value\n",
    "        \n",
    "\n",
    "    # if email text or label is missed, it is not worth to keep the data, we simply remove that from initially imported set.\n",
    "\n",
    "    else:\n",
    "      imported_data.drop( temp_data[0],inplace=True) \n",
    "\n",
    "#### Part 1.3 : Data Formatting for Machine Learning\n",
    "#####################################################\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# converting to lower case \n",
    "imported_data.iloc[:,2]=imported_data.iloc[:,2].astype(str)\n",
    "print(imported_data.iloc[:,2].dtype)\n",
    "imported_data.iloc[:,2]=imported_data.iloc[:,2].apply(lambda x: x.lower())\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "cleaned_texts = []\n",
    "flow_labels_=[]\n",
    "context_labels_=[]\n",
    "\n",
    "\n",
    "cleaned_texts = []\n",
    "flow_labels_ = []\n",
    "context_labels_ = []\n",
    "\n",
    "for text in imported_data.iloc[:, 2]:\n",
    "    # Step 1: Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Step 2: Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Step 3: Remove stop words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Step 4: Join cleaned words back into a single string\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "    \n",
    "    # Step 5: Label contexts\n",
    "    context_labels = [\n",
    "        context for context, keywords in contexts.items() \n",
    "        if any(keyword in cleaned_text.lower() for keyword in keywords)\n",
    "    ]\n",
    "    # Pick the context with the most matches if at least one was found\n",
    "    if context_labels:\n",
    "        context_labels = [\n",
    "            max(\n",
    "                context_labels,\n",
    "                key=lambda c: sum(keyword in cleaned_text.lower() for keyword in contexts[c])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Step 6: Label logical flows\n",
    "    flow_labels = [\n",
    "        flow for flow, keywords in logical_flows.items() \n",
    "        if any(keyword in cleaned_text.lower() for keyword in keywords)\n",
    "    ]\n",
    "    # Pick the flow with the most matches if at least one was found\n",
    "    if flow_labels:\n",
    "        flow_labels = [\n",
    "            max(\n",
    "                flow_labels,\n",
    "                key=lambda f: sum(keyword in cleaned_text.lower() for keyword in logical_flows[f])\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    # Store results\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "    flow_labels_.append(flow_labels)\n",
    "    context_labels_.append(context_labels)\n",
    "\n",
    "# Attach these new columns to the DataFrame\n",
    "imported_data[\"cleaned_text\"]   = cleaned_texts\n",
    "imported_data[\"contexts\"]       = context_labels_\n",
    "imported_data[\"logical_flows\"]  = flow_labels_\n",
    "\n",
    "print(imported_data.head())\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. Convert each feature column into a numeric form\n",
    "# ------------------------------------------------\n",
    "\n",
    "# A. cleaned_text -> Bag-of-Words\n",
    "vectorizer_text = CountVectorizer()\n",
    "X_text = vectorizer_text.fit_transform(imported_data[\"cleaned_text\"])\n",
    "# X_text is sparse (n_samples x vocab_size_from_cleaned_text)\n",
    "\n",
    "# B. contexts -> MultiLabel Binarization\n",
    "mlb_context = MultiLabelBinarizer()\n",
    "# contexts is a list of labels per row\n",
    "X_context = mlb_context.fit_transform(imported_data[\"contexts\"])\n",
    "# X_context is dense (n_samples x num_unique_contexts)\n",
    "\n",
    "# C. logical_flows -> MultiLabel Binarization\n",
    "mlb_flows = MultiLabelBinarizer()\n",
    "# logical_flows is a list of flow labels per row\n",
    "X_flows = mlb_flows.fit_transform(imported_data[\"logical_flows\"])\n",
    "# X_flows is dense (n_samples x num_unique_flows)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Combine all numeric features side by side\n",
    "# ---------------------------------------------\n",
    "# If you prefer everything in sparse format, convert the dense arrays:\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_context_sparse = csr_matrix(X_context)\n",
    "X_flows_sparse   = csr_matrix(X_flows)\n",
    "\n",
    "# Horizontally stack them\n",
    "X = hstack([X_text, X_context_sparse, X_flows_sparse])\n",
    "\n",
    "# Your label column (assuming column index 3 is your target)\n",
    "Y = imported_data.iloc[:, 3]\n",
    "\n",
    "# Now X and Y have matching sample sizes (rows).\n",
    "print(f\"X shape = {X.shape}, Y shape = {Y.shape}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Train/test split, train model, and evaluate\n",
    "# ----------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
